import cgi
import cPickle as pickle
from datetime import datetime, timedelta
import gzip
import md5
from nuus.database import CachedDatabase
import os
from pyelasticsearch import ElasticSearch, exceptions as esexcep
import re
import time
from utils import parse_date

class Segment(object):
    def __eq__(self, s2):
        if s2 is None or not isinstance(s2, Segment):
            return False
        return self.number == s2.number and self.bytes == s2.bytes and self.id == s2.id
    def __init__(self, number, bytes, id):
        self.number = number
        self.bytes = bytes
        self.id = id
    def __repr__(self):
        return "%s, %s, %s" % (self.number, self.bytes, self.id)
    def nzb(self):
        return '<segment bytes="%s" number="%s">%s</segment>' % (
            self.bytes, self.number, self.id)
    def todict(self):
        return dict(number=self.number,bytes=self.bytes,id=self.id)

class File(object):
    def __init__(self, name, total_parts, poster, id=None, date=None, subject=None):
        self.name = name
        self.total_parts = total_parts
        self.poster = poster
        self.date = date
        self.subject = subject
        self.groups = set()
        self.segments = [None] * total_parts
    def __repr__(self):
        return "FILE<%s,%s/%s>" % (
            self.name, len(filter(None, self.segments)), self.total_parts)
    def complete(self):
        return all(self.segments)
    def insert_part(self, group, article_id, part, size, subject=None, date=None):
        # TODO: the 'bytes' (i.e. size) value i get seems to be different from what nzbindex.nl gives
        # sabnzbd doesn't seem to care though, so whatever!
        if part > self.total_parts:
            raise ValueError("Got part greater than max part number")
        s = Segment(part, size, article_id)
        if self.segments[part-1] is not None:
            if s != self.segments[part-1]:
                pass # NOTE: nzbindex.nl seems to just take the first entry, so that's what i'm gonna do!
        else:
            self.segments[part-1] = s
            self.groups.add(group)
            # set subject to the same values as found in the first article for the file
            if part == 1:
                if subject is not None:
                    self.subject = subject
                if date is not None:
                    self.date = date
                self.id = article_id
    def nzb(self):
        if not self.complete():
            raise ValueError("cannot create nzb for incomplete file")
        nzb = '<file poster="%s" date="%s" subject="%s">\n' % (
            cgi.escape(self.poster, quote=True), self.date, cgi.escape(self.subject, quote=True))
        nzb += '<groups>\n'
        for g in self.groups:
            nzb += '<group>%s</group>\n' % g
        nzb += '</groups>\n<segments>\n'
        for s in self.segments:
            nzb += s.nzb() + '\n'
        nzb += '</segments>\n</file>'
        return nzb
    def todict(self):
        return dict(name=self.name,
                    total_parts=self.total_parts,
                    poster=self.poster,
                    date=self.date,
                    subject=self.subject,
                    groups=list(self.groups),
                    segments=[x.todict() for x in self.segments])

class Release(object):
    def __init__(self, name, total_parts, nfo=None, nzb=None):
        self.name = name
        self.total_parts = total_parts
        self.nfo = nfo
        self.files = [None] * total_parts
        self.zero = nzb
    def __repr__(self):
        return "RELEASE<%s,%s/%s>" % (
            self.name, len(filter(None, self.files)), self.total_parts)
    def add_file(self, id, part, is_nfo=False):
        if part == 0:
            # TODO: are all 0/0 for releases nzbs ?
            self.zero = id
        self.files[part-1] = id
        if is_nfo:
            self.nfo = id
    def complete(self):
        return all(self.files)
    def todict(self):
        return dict(name=self.name,files=self.files,nfo=self.nfo,zero=self.zero)

nzb_prefix = """<?xml version="1.0" encoding="iso-8859-1" ?>
<!DOCTYPE nzb PUBLIC "-//newzBin//DTD NZB 1.0//EN" "http://www.nzbindex.com/nzb-1.0.dtd">
<!-- NZB Generated by Nuus -->
<nzb xmlns="http://www.newzbin.com/DTD/2003/nzb">"""
nzb_suffix = """</nzb>
"""

# UTILS
def _write_nzb_for_file(f):
    """http://wiki.sabnzbd.org/nzb-specs"""
    try:
        os.makedirs('nzbs')
    except OSError:
        pass
    osfn = os.path.join('nzbs',f.name + '.nzb')
    if os.path.exists(osfn):
        print osfn, 'already exists'
    else:
        osf = open(osfn, 'w')
        osf.write(nzb_prefix + '\n')
        osf.write(f.nzb() + '\n')
        osf.write(nzb_suffix)
        osf.close()

def _gen_idx_key(*args):
    """hashes enough details about a file in hopes it will be unique"""
    m = md5.new()
    for a in args:
        if a : m.update(str(a))
    return m.hexdigest()

class Parser(object):
    def __init__(self, clean_index=False, patterns=[]):
        #self._releases = dict()
        #self._files = dict()
        self._db = CachedDatabase('parser.db', cached_entries=10000, auto_flush=False)
        self._patterns = patterns
        self._errorlog = gzip.open('error.log.gz', 'w')
        self._es = ElasticSearch('http://localhost:9200')
        if clean_index:
            try:
                self._es.delete_index('nuus')
            except esexcep.ElasticHttpNotFoundError as e:
                if not e.error.startswith('IndexMissingException'):
                    raise

    def process_article(self, group, post_number, subject, poster, 
                        date, article_id, references, size, lines):
        if subject is None:
            return
        # fix up encodings
        try:
            subject = subject.decode('latin-1').encode('utf-8')
        except:
            print 'failed to decode "%r"' % subject
            # then just try to process it anyway
        for p in self._patterns:
            m = p.match(subject)
            if m:
                m = m.groupdict() # allow us to .get groups that don't exist (and thus get None for those values)
                release_name, release_part, release_total, file_name, file_part, file_total = (
                    m.get('release_name'), m.get('release_part'), m.get('release_total'),
                    m.get('file_name'), m.get('file_part'), m.get('file_total')
                )
                filekey = _gen_idx_key(release_name, release_part, release_total, file_name, file_total, poster)
                file = self._db.get(filekey)
                if file is None:
                    file = File(file_name, int(file_total), poster)
                if file.complete():
                    return # skip if it's already complete
                file.insert_part(group, article_id[1:-1], int(file_part), int(size), subject, parse_date(date))
                self._db.set(filekey, file)
                if file.complete():
                    print '===', file
                    res = self._es.index('nuus', 'file', file.todict())
                    file_id = res.get('_id')
                    if release_name:
                        rlskey = _gen_idx_key(release_name, release_total)
                        release = self._db.get(rlskey)
                        if release is None:
                            release = Release(release_name, int(release_total))
                        if release.complete():
                            return # skip if completed
                        release.add_file(file_id, int(release_part), file_name.endswith('.nfo'))
                        self._db.set(rlskey, release)
                        if release.complete():
                            print '+++', release
                            self._es.index('nuus', 'release', release.todict())
                            self._db.delete(rlskey)
                return
        self._errorlog.write('%s\n' % subject)
        self._errorlog.flush()
        # TODO: log unparsable file name

if __name__ == '__main__':
    f = open('patterns.txt')
    parser = Parser(clean_index=True, patterns=map(re.compile,filter(lambda x: not (x == '' or x.startswith('#')), [x.strip() for x in f.readlines()])))
    f.close()
    for group in os.listdir('cache'):
        for page in os.listdir(os.path.join('cache',group)):
            print 'parsing', group, page
            f = gzip.open(os.path.join('cache',group,page), 'r')
            articles = pickle.load(f)
            f.close()
            for article in articles:
                parser.process_article(group, *article)
